{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Snowpark Imports\n",
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark import types as T\n",
    "from snowflake.snowpark.window import Window\n",
    "\n",
    "# Snowflake Python API\n",
    "from snowflake.core import Root\n",
    "from snowflake.core.database import Database\n",
    "from snowflake.core.warehouse import Warehouse\n",
    "from snowflake.core.service import Service, ServiceSpecStageFile\n",
    "from snowflake.core.compute_pool import ComputePool\n",
    "from snowflake.core.image_repository import ImageRepository\n",
    "\n",
    "# Other\n",
    "from PIL import Image\n",
    "from pprint import pprint\n",
    "from datasets import load_dataset\n",
    "import concurrent.futures\n",
    "import io\n",
    "import itertools\n",
    "import threading\n",
    "from plotting.image_plotting import plot_similar_images, plot_image_grid\n",
    "from plotting.image_cluster_plotting import visualize_image_clusters\n",
    "from api_calls.embedding_service import get_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Connect to Snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "snowflake_connection_cfg = {\n",
    "    \"ACCOUNT\": os.getenv('SF_ACCOUNT'),\n",
    "    \"USER\": os.getenv('SF_USER'),\n",
    "    \"ROLE\": os.getenv('SF_ROLE'),\n",
    "    \"PASSWORD\": os.getenv('SF_PASSWORD'),\n",
    "}\n",
    "\n",
    "# Creating Snowpark Session\n",
    "session = Session.builder.configs(snowflake_connection_cfg).create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Set Up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Create Database & Schema\n",
    "root = Root(session)\n",
    "demo_db = Database(name=\"REVERSE_IMAGE_SEARCH\")\n",
    "demo_db = root.databases.create(demo_db, mode='if_not_exists')\n",
    "\n",
    "# Create warehouse\n",
    "wh = Warehouse(name=\"COMPUTE_WH\", warehouse_size=\"XSMALL\", auto_suspend=600, auto_resume='true')\n",
    "warehouses = root.warehouses\n",
    "wh = warehouses.create(wh, mode='if_not_exists')\n",
    "\n",
    "# Set context\n",
    "session.use_schema('REVERSE_IMAGE_SEARCH.PUBLIC')\n",
    "session.use_warehouse('COMPUTE_WH')\n",
    "\n",
    "# Create a Snowflake Stage for Images\n",
    "session.sql(\"\"\"CREATE STAGE IF NOT EXISTS IMAGES\n",
    "                DIRECTORY = (ENABLE = TRUE AUTO_REFRESH = FALSE) \n",
    "                ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE') \n",
    "                COMMENT='Stage to store Image Files'\"\"\").collect()\n",
    "\n",
    "# Create a Snowflake Stage for Container Files (Spec-Files & Models)\n",
    "session.sql(\"\"\"CREATE STAGE IF NOT EXISTS CONTAINER_FILES\n",
    "                DIRECTORY = (ENABLE = TRUE AUTO_REFRESH = FALSE) \n",
    "                ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE') \n",
    "                COMMENT='Stage to store Container Files'\"\"\").collect()\n",
    "\n",
    "# Create a an External Access Integration (to download models from HuggingFace)\n",
    "session.sql(\"\"\"CREATE OR REPLACE NETWORK RULE hf_rule\n",
    "                MODE= 'EGRESS'\n",
    "                TYPE = 'HOST_PORT'\n",
    "                VALUE_LIST = (\n",
    "                    'huggingface.co',\n",
    "                    'cdn-lfs-us-1.huggingface.co',\n",
    "                    'cdn-lfs.huggingface.co')\"\"\").collect()\n",
    "\n",
    "session.sql(\"\"\"CREATE OR REPLACE EXTERNAL ACCESS INTEGRATION hf_integration\n",
    "                ALLOWED_NETWORK_RULES = (hf_rule)\n",
    "                ENABLED = true\"\"\").collect()\n",
    "\n",
    "# Create an Image Repository\n",
    "new_image_repository = ImageRepository(name=\"image_repository\")\n",
    "image_repositories = root.databases[\"REVERSE_IMAGE_SEARCH\"].schemas[\"PUBLIC\"].image_repositories\n",
    "my_image_repo = image_repositories.create(new_image_repository, mode='if_not_exists')\n",
    "\n",
    "# Create a compute pool for the Image Embedding Model\n",
    "compute_pool_def = ComputePool(\n",
    "    name=\"GPU_POOL\",\n",
    "    instance_family=\"GPU_NV_S\",\n",
    "    min_nodes=1,\n",
    "    max_nodes=1\n",
    ")\n",
    "my_compute_pool = root.compute_pools.create(compute_pool_def, mode='if_not_exists')\n",
    "\n",
    "# Create a compute pool for the Streamlit App\n",
    "compute_pool_def2 = ComputePool(\n",
    "    name=\"CPU_POOL\",\n",
    "    instance_family=\"CPU_X64_XS\",\n",
    "    min_nodes=1,\n",
    "    max_nodes=1\n",
    ")\n",
    "my_compute_pool2 = root.compute_pools.create(compute_pool_def2, mode='if_not_exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Create the Embedding Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "session.file.put('spcs/embedding_service/container_spec.yml', stage_location='@CONTAINER_FILES', overwrite=True, auto_compress=False)\n",
    "\n",
    "service_def = Service(\n",
    "    name=\"EMBEDDING_SERVICE\",\n",
    "    compute_pool=\"GPU_POOL\",\n",
    "    spec=ServiceSpecStageFile(spec_file='container_spec.yml', stage='CONTAINER_FILES'),\n",
    "    min_instances=1,\n",
    "    max_instances=1,\n",
    "    external_access_integrations=['HF_INTEGRATION']\n",
    ")\n",
    "\n",
    "embedding_service = demo_db.schemas['PUBLIC'].services.create(service_def, mode='if_not_exists')\n",
    "\n",
    "# Create a Function to call the embedding function\n",
    "session.sql(\"\"\"CREATE OR REPLACE FUNCTION GENERATE_IMAGE_EMBEDDING(IMAGE_URL TEXT)\n",
    "                RETURNS ARRAY\n",
    "                SERVICE = EMBEDDING_SERVICE\n",
    "                ENDPOINT=API\n",
    "                AS '/generate-embeddings'\"\"\").collect()\n",
    "\n",
    "# Get the Service Status\n",
    "print('SERVICE STATUS:')\n",
    "pprint(embedding_service.get_service_status())\n",
    "\n",
    "# Get the Service Logs\n",
    "print('\\nSERVICE LOGS:')\n",
    "print(embedding_service.get_service_logs(container_name='dinov2-base-service-container', instance_id='0'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Upload Images to Snowflake\n",
    "\n",
    "We will use the `ceyda/fashion-products-small` dataset from Hugging Face with over 42.000 images.  \n",
    "For demo purposes we sample (10%) and filter the dataset to only include images with the following categories: ['Footwear']  \n",
    "The following cell will download the dataset, sample it, and then upload the files to Snowflake in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ceyda/fashion-products-small\", split='train')\n",
    "# Sample 10% of the dataset\n",
    "sampled_dataset = dataset.train_test_split(test_size=0.1)['test']\n",
    "# Filter to only include Footwear Images\n",
    "sampled_dataset = sampled_dataset.filter(lambda example: example[\"masterCategory\"] == 'Footwear')\n",
    "print(f'Sampled {sampled_dataset.num_rows} Images.')\n",
    "\n",
    "# Create a thread-safe counter\n",
    "upload_counter = itertools.count(1)  # Starts counting from 1\n",
    "print_lock = threading.Lock()\n",
    "\n",
    "thread_snowflake_connection_cfg = snowflake_connection_cfg\n",
    "thread_snowflake_connection_cfg['DATABASE'] = 'REVERSE_IMAGE_SEARCH'\n",
    "thread_snowflake_connection_cfg['SCHEMA'] = 'PUBLIC'\n",
    "thread_snowflake_connection_cfg['WAREHOUSE'] = 'COMPUTE_WH'\n",
    "\n",
    "# Function to upload data using a provided session\n",
    "def upload_data(row, session):\n",
    "    \"\"\"Saves each image to a Snowflake Stage\"\"\"\n",
    "    file_name = row['filename']\n",
    "    img_byte_arr = io.BytesIO()\n",
    "    row['image'].save(img_byte_arr, format='JPEG')\n",
    "    session.file.put_stream(input_stream=img_byte_arr, stage_location=f'@IMAGES/{file_name}', auto_compress=False, overwrite=False)\n",
    "\n",
    "    # Thread-safe increment of the counter\n",
    "    with print_lock:\n",
    "        current_count = next(upload_counter)\n",
    "        if current_count % 100 == 0:\n",
    "            print(f'{current_count} images from {sampled_dataset.num_rows} uploaded ...')\n",
    "    return file_name\n",
    "\n",
    "# Wrapper function to handle the creation of session and the actual uploading\n",
    "# Snowpark Session are not thread-safe, so creating one session per worker\n",
    "def upload_with_session(row):\n",
    "    with Session.builder.configs(thread_snowflake_connection_cfg).create() as session:\n",
    "        return upload_data(row, session)\n",
    "\n",
    "# Function to parallelize uploads with session management\n",
    "def parallel_upload(dataset):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        # Map upload_with_session to each row of the dataset\n",
    "        results = list(executor.map(upload_with_session, dataset))\n",
    "    return results\n",
    "\n",
    "# Execute the parallel upload\n",
    "upload_results = parallel_upload(sampled_dataset)\n",
    "# Refresh the stage to register new files\n",
    "session.sql('ALTER STAGE IMAGES REFRESH').show()\n",
    "\n",
    "print(f'Uploaded {len(upload_results)} files.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Generate Image Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame with Images\n",
    "images_df = session.sql(\"SELECT * FROM DIRECTORY('@IMAGES')\")\n",
    "\n",
    "# Generate an accessible URL and apply the embedding function on images\n",
    "images_df = images_df.with_column('PRESIGNED_URL', F.call_builtin('GET_PRESIGNED_URL', '@IMAGES', F.col('RELATIVE_PATH')))\n",
    "images_df = images_df.with_column('IMAGE_EMBEDDING', F.call_builtin('GENERATE_IMAGE_EMBEDDING', F.col('PRESIGNED_URL')).cast(T.VectorType(float,768)))\n",
    "images_df.write.save_as_table('FASHION_IMAGES_EMBEDDINGS', mode='overwrite')\n",
    "\n",
    "images_df = session.table('FASHION_IMAGES_EMBEDDINGS')\n",
    "images_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a helper function, we can easily visualize image clusters\n",
    "visualize_image_clusters(images_df[['PRESIGNED_URL','RELATIVE_PATH','IMAGE_EMBEDDING']].sample(n=50).to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - Calculate the Similarity between all Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RETURN_TOP_N = 5 # number of similar images to return per image\n",
    "window = Window.partition_by(['RELATIVE_PATH_LEFT']).order_by(F.col('COSINE_SIMILARITY').desc())\n",
    "crossjoin_images_df = images_df.cross_join(images_df, rsuffix='_RIGHT', lsuffix='_LEFT')\n",
    "crossjoin_images_df = crossjoin_images_df.with_column('COSINE_SIMILARITY', F.call_builtin('VECTOR_COSINE_SIMILARITY', F.col('IMAGE_EMBEDDING_LEFT'), F.col('IMAGE_EMBEDDING_RIGHT')))\n",
    "crossjoin_images_df = crossjoin_images_df.select('RELATIVE_PATH_LEFT','RELATIVE_PATH_RIGHT','COSINE_SIMILARITY','PRESIGNED_URL_RIGHT','PRESIGNED_URL_LEFT')\n",
    "crossjoin_images_df = crossjoin_images_df.with_column('ROW', F.row_number().over(window)).filter(F.col('ROW')<=RETURN_TOP_N)\n",
    "crossjoin_images_df = crossjoin_images_df.with_column('COSINE_SIMILARITY', F.round('COSINE_SIMILARITY', 2))\n",
    "crossjoin_images_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample N images for visualization\n",
    "SAMPLE_SIZE = 5\n",
    "sample_images = [row['RELATIVE_PATH_LEFT'] for row in crossjoin_images_df.select('RELATIVE_PATH_LEFT').distinct().sample(n=SAMPLE_SIZE).collect()]\n",
    "viz_images = crossjoin_images_df.filter(F.col('RELATIVE_PATH_LEFT').in_(sample_images))\n",
    "# Regenerate URLs in case they are not valid anymore\n",
    "viz_images = viz_images.with_column('PRESIGNED_URL_LEFT', F.call_builtin('GET_PRESIGNED_URL', '@IMAGES', F.col('RELATIVE_PATH_LEFT')))\n",
    "viz_images = viz_images.with_column('PRESIGNED_URL_RIGHT', F.call_builtin('GET_PRESIGNED_URL', '@IMAGES', F.col('RELATIVE_PATH_RIGHT')))\n",
    "df = viz_images.to_pandas()\n",
    "# Visualize the similar images\n",
    "plot_similar_images(df, RETURN_TOP_N=RETURN_TOP_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 - Query with given image (Reverse Image Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'sample_images/sneaker1.jpg'\n",
    "print('Query Image:')\n",
    "display(Image.open(filename).resize((100,100)))\n",
    "# Retrieve the embedding\n",
    "embedding = get_embedding(session=session, filename=filename)\n",
    "# Search image database\n",
    "search_df = images_df.with_column('QUERY_EMBEDDING', F.lit(embedding[0].tolist()).cast(T.VectorType(float,768)))\n",
    "search_df = search_df.with_column('COSINE_SIMILARITY', F.call_builtin('VECTOR_COSINE_SIMILARITY', F.col('QUERY_EMBEDDING'), F.col('IMAGE_EMBEDDING')))\n",
    "search_df = search_df.order_by(F.col('COSINE_SIMILARITY').desc())\n",
    "search_df = search_df.with_column('COSINE_SIMILARITY', F.round('COSINE_SIMILARITY', 2))\n",
    "search_df = search_df.with_column('PRESIGNED_URL', F.call_builtin('GET_PRESIGNED_URL', '@IMAGES', F.col('RELATIVE_PATH'))).cache_result()\n",
    "search_df[['RELATIVE_PATH','COSINE_SIMILARITY']].show(5)\n",
    "# Visualize the similar images\n",
    "plot_image_grid(search_df.limit(10).to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 - Create a Search Service App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql('DROP SERVICE SEARCH_SERVICE').show()\n",
    "\n",
    "session.file.put('spcs/streamlit_app/streamlit_container_spec.yml', stage_location='@CONTAINER_FILES', overwrite=True, auto_compress=False)\n",
    "\n",
    "search_service_def = Service(\n",
    "    name=\"SEARCH_SERVICE\",\n",
    "    compute_pool=\"CPU_POOL\",\n",
    "    spec=ServiceSpecStageFile(spec_file='streamlit_container_spec.yml', stage='CONTAINER_FILES'),\n",
    "    min_instances=1,\n",
    "    max_instances=1\n",
    ")\n",
    "\n",
    "search_service = demo_db.schemas['PUBLIC'].services.create(search_service_def, mode='if_not_exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = session.sql('SHOW ENDPOINTS IN SERVICE SEARCH_SERVICE').collect()[0]['ingress_url']\n",
    "if endpoint.startswith('Endpoints provisioning in progress...'):\n",
    "    print(endpoint)\n",
    "else:\n",
    "    print('URL to Streamlit App:')\n",
    "    print(f\"https://{endpoint}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysnowpark_huggingface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
